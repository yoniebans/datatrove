# Example 5: Deduplication Pipeline

## Objective
Remove duplicate and near-duplicate content using hash-based exact deduplication.

## Components
- **JsonlReader**: Read documents from synthetic test data
- **LambdaFilter**: Hash-based exact duplicate detection and removal
- **JsonlWriter**: Save deduplicated results

## Implementation
**File:** `spec/phase1/examples/05_deduplication.py`

## Data Requirements
- **Input:** `spec/phase1/data/sample_with_duplicates.jsonl` (generated by script)
- **Output:** `spec/phase1/output/05_dedup_hash/*.jsonl`
- **Logs:** `spec/phase1/logs/05_dedup_hash/`
- **Optional C4 test:** `hf://datasets/allenai/c4/en/` (glob: `c4-train.00000-of-01024.json.gz`, limit: 5000)

## Expected Results
- Synthetic data: 14 â†’ 10 documents (28.6% duplicates removed)
- Test data contains: 5 unique, 4 exact duplicates, 4 near duplicates, 1 duplicate pangram
- Exact duplicates removed, near duplicates kept
- C4 test: 0 duplicates found (C4 is pre-deduplicated)

## Status
- [x] Implemented
- [x] Tested
- [x] Documentation updated

## Notes
- Uses SHA-256 hash for exact duplicate detection
- Hash-based dedup is simple and efficient for exact matches
- Near-duplicate detection (MinHash) demonstrated but requires complex multi-stage setup
- BloomFilter and MinHash better suited for distributed environments (Phase 2)